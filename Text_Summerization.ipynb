{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Text Summarization**\n",
        "\n"
      ],
      "metadata": {
        "id": "jLq4QxWQOTTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1- Web scraping for Obtaining Data for Summarization** \n",
        "###I will obtain data from the URL using the Web scraping. For scrapping, i'll use the beautifulsoup library, that will be used to fetch the data on the web page within the various HTML tags.\n",
        "###I will try to summarize the Reinforcement Learning page on Wikipedia.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tpKOz9ybLVWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 # Install the beautifulsoup library in Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1eElYraLzwQ",
        "outputId": "8eeac161-70a6-4b39-bcdb-ab1f02e04ac3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lxml # To parse the HTML tags we will further require a parser, that is the lxml package:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y9XY310N-KM",
        "outputId": "08d9cfbb-2c7a-4703-f23f-5f839519400d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.9.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Reinforcement_learning')\n",
        "article = data.read()\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "raw_text = \"\"\n",
        "for p in paragraphs:\n",
        "    raw_text += p.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfxmoPWJ2qFC",
        "outputId": "31212081-e962-4937-ccd5-c92fdcd55476"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2- Text Preprocessing**\n",
        "###The first task is to remove all the references made in the Wikipedia article, which are all enclosed in square brackets, and replace them with spaces."
      ],
      "metadata": {
        "id": "t4bK324ViaR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I will not removing any other words or punctuation marks as I will use them directly to create the summaries.\n"
      ],
      "metadata": {
        "id": "mzFmBErClo79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Square Brackets and Extra Spaces\n",
        "raw_text = re.sub(r'\\[[0-9]*\\]', ' ', raw_text)  \n",
        "raw_text = re.sub(r'\\s+', ' ', raw_text)  \n",
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', raw_text)  \n",
        "formatted_article_text = re.sub(r's+', ' ', formatted_article_text)"
      ],
      "metadata": {
        "id": "z-P3Z472nks9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3- Convert paragraphs to sentences**\n"
      ],
      "metadata": {
        "id": "_GkRZYfwoFSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sentence_list = nltk.sent_tokenize(raw_text)  "
      ],
      "metadata": {
        "id": "eKqFy94SnvFR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I tokenized the article_text object as it is unfiltered data while the formatted_article_text object has formatted data devoid of punctuations etc.\n",
        "#**4- Tokenizing the sentences**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T48xLCwSp60F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nARVcOisqkVz",
        "outputId": "c011a1a4-4687-405c-f394-6c5980d81c11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5- Finding weighted frequencies of occurrence**"
      ],
      "metadata": {
        "id": "c4h-mHb2rVMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}  \n",
        "for word in nltk.word_tokenize(formatted_article_text):  \n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "metadata": {
        "id": "aSFwe3zMq51e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5- Find weighted frequency of occurrence**\n",
        "##To find the weighted frequency, divide the frequency of the word by the frequency of the most occurring word.\n"
      ],
      "metadata": {
        "id": "t0jxMy0Btd_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies.keys():  \n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "metadata": {
        "id": "VBcfRvihsWze"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6- Calculate sentence scores**\n",
        "We have calculated the weighted frequencies. Now scores for each sentence can be calculated by adding weighted frequencies for each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "N-k_Oq_nvjfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}  \n",
        "for sent in sentence_list:  \n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "metadata": {
        "id": "MF2wUSidvlID"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7- Summarizing the Article**\n",
        "###The sentence_scores dictionary consists of the sentences along with their scores. Now, top N sentences can be used to form the summary of the article.\n",
        "###Here the heapq library has been used to pick the top 7 sentences to summarize the article.\n",
        "\n"
      ],
      "metadata": {
        "id": "J2TACUM0m2Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "summary = ' '.join(summary_sentences)\n",
        "print(summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9EFkD3gxCYR",
        "outputId": "d9bbf8ae-79a9-45ef-f5ab-b6f185449477"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning. Research topics include Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces. Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scAOp9kV4sdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}